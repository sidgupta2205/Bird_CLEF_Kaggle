{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11215324,"sourceType":"datasetVersion","datasetId":7003445},{"sourceId":11241196,"sourceType":"datasetVersion","datasetId":7023215}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T13:11:33.798619Z","iopub.execute_input":"2025-04-03T13:11:33.798839Z","iopub.status.idle":"2025-04-03T13:11:34.747340Z","shell.execute_reply.started":"2025-04-03T13:11:33.798818Z","shell.execute_reply":"2025-04-03T13:11:34.746543Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\n\nimport torch.optim as optim\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport os\nimport random\nimport numpy as np\nfrom PIL import Image\nimport torch.nn.functional as F\n\n# Define transformation for spectrogram images\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n# Dataset to load spectrograms and apply masking\nclass SpectrogramDataset(Dataset):\n    def __init__(self, folder_path, transform=None, mask_ratio=0.5, patch_size=16):\n        self.folder_path = folder_path\n        self.transform = transform\n        self.mask_ratio = mask_ratio\n        self.patch_size = patch_size\n        self.image_paths = self.image_paths = [\n            os.path.join(root, f) \n            for root, _, files in os.walk(folder_path) \n            for f in files if f.endswith('.png')\n        ]\n\n    import torch.nn.functional as F\n\n    import torch.nn.functional as F\n\n    def mask_spectrogram(self, img):\n        \"\"\"Randomly mask patches of the spectrogram\"\"\"\n        c, h, w = img.shape\n        num_patches_h = h // self.patch_size\n        num_patches_w = w // self.patch_size\n    \n        # Create binary mask with patches\n        mask = torch.ones((num_patches_h, num_patches_w))\n        num_masked = int(self.mask_ratio * num_patches_h * num_patches_w)\n        masked_indices = random.sample(range(num_patches_h * num_patches_w), num_masked)\n    \n        for idx in masked_indices:\n            i, j = divmod(idx, num_patches_w)\n            mask[i, j] = 0  # Set masked patches to 0\n    \n        # Resize mask to match image size\n        mask = mask.unsqueeze(0).unsqueeze(0)  # Shape (1,1,H,W)\n        mask = F.interpolate(mask, size=(h, w), mode=\"bilinear\", align_corners=False)  # Resize smoothly\n        mask = mask.squeeze(0).squeeze(0)  # Remove extra dimensions\n    \n        return img * mask, mask\n\n\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n        if self.transform:\n            img = self.transform(img)\n        masked_img, mask = self.mask_spectrogram(img)\n        return masked_img, img, mask  # Masked spectrogram, original spectrogram, mask\n\n# Define EfficientNet-B0 based encoder\nclass EfficientNetMaskedModel(nn.Module):\n    def __init__(self, pretrained=True):\n        super(EfficientNetMaskedModel, self).__init__()\n        self.encoder = models.efficientnet_b0(pretrained=pretrained)\n        \n        # Modify first convolution layer to accept 1-channel input\n        self.encoder.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        \n        self.encoder.classifier = nn.Identity()  # Remove classification head\n        \n        self.decoder = nn.Sequential(\n            nn.Conv2d(1280, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # 7x7 → 14x14\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),  # 14x14 → 28x28\n            nn.ReLU(),\n            nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1),   # 28x28 → 56x56\n            nn.ReLU(),\n            nn.ConvTranspose2d(8, 4, kernel_size=4, stride=2, padding=1),    # 56x56 → 112x112\n            nn.ReLU(),\n            nn.ConvTranspose2d(4, 1, kernel_size=4, stride=2, padding=1)     # 112x112 → 224x224\n        )\n\n\n    def forward(self, x):\n        encoded = self.encoder.features(x)  # Extract features\n        reconstructed = self.decoder(encoded)  # Reconstruct masked spectrogram\n        return reconstructed\n\n\n\n# Training setup with validation loop\ndef train_model(data_folder, epochs=10, batch_size=16, lr=1e-3, val_split=0.2):\n    dataset = SpectrogramDataset(data_folder, transform)\n    \n    # Split dataset into train and validation sets\n    train_size = int((1 - val_split) * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # model = EfficientNetMaskedModel(pretrained=True).cuda()\n    model = trained_model\n    criterion = nn.MSELoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n\n    best_val_loss = float('inf')\n\n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        train_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n        for masked_spectrograms, original_spectrograms, _ in progress_bar:\n            masked_spectrograms, original_spectrograms = masked_spectrograms.cuda(), original_spectrograms.cuda()\n            optimizer.zero_grad()\n            reconstructed = model(masked_spectrograms)\n            # print(reconstructed.shape,original_spectrograms.shape)\n            loss = criterion(reconstructed, original_spectrograms)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n            progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n\n        train_loss /= len(train_loader)\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for masked_spectrograms, original_spectrograms, _ in val_loader:\n                masked_spectrograms, original_spectrograms = masked_spectrograms.cuda(), original_spectrograms.cuda()\n                reconstructed = model(masked_spectrograms)\n                loss = criterion(reconstructed, original_spectrograms)\n                val_loss += loss.item()\n\n        val_loss /= len(val_loader)\n        clear_output()\n        if (epoch%3==0) and (epoch!=0):\n            print(f\"Saving Checkpoint for {epoch}\")\n            torch.save(model.state_dict(), f\"best_model_{epoch}.pth\")\n        # Save best model\n        # if val_loss < best_val_loss:\n        #     best_val_loss = val_loss\n        #     torch.save(model.state_dict(), \"best_model.pth\")\n        #     print(f\"Best model saved at epoch {epoch + 1}\")\n        \n        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n\n    return model\n\n# Run training\ndata_folder = \"/kaggle/input/spectograms-5-sec-cut/\"\ntrained_model = train_model(data_folder,batch_size = 128)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T15:54:47.571145Z","iopub.execute_input":"2025-04-03T15:54:47.571428Z","iopub.status.idle":"2025-04-03T18:35:37.368811Z","shell.execute_reply.started":"2025-04-03T15:54:47.571408Z","shell.execute_reply":"2025-04-03T18:35:37.367858Z"}},"outputs":[{"name":"stdout","text":"Saving Checkpoint for 9\nEpoch 10/10, Train Loss: 0.006944, Val Loss: 0.006803\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}